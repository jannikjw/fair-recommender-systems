{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lyzbdeoNVB7V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from importlib import reload\n",
    "from trecs.metrics import MSEMeasurement, InteractionSpread, InteractionSpread, InteractionSimilarity, RecSimilarity, RMSEMeasurement, InteractionMeasurement\n",
    "from trecs.components import Users\n",
    "import trecs.matrix_ops as mo\n",
    "import src.globals as globals\n",
    "import seaborn as sns\n",
    "\n",
    "from wrapper.models.bubble import BubbleBurster\n",
    "from src.utils import get_topic_clusters, create_embeddings, load_or_create_measurements_df, load_and_process_movielens, collect_parameters, load_measurements\n",
    "from src.scoring_functions import cosine_sim, entropy, content_fairness\n",
    "from wrapper.metrics.evaluation_metrics import SerendipityMetric, DiversityMetric, NoveltyMetric, RecallMeasurement\n",
    "\n",
    "random_state = np.random.seed(42)\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "globals.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating RecommenderSystem on MovieLens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_attrs=20\n",
    "max_iter=1000\n",
    "n_clusters=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_ratings_matrix = load_and_process_movielens(file_path='data/ml-100k/u.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Get user and item representations using NMF\n",
    "user_representation, item_representation = create_embeddings(binary_ratings_matrix, n_attrs=n_attrs, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded clusters.\n"
     ]
    }
   ],
   "source": [
    "# Define topic clusters using NMF\n",
    "item_topics = get_topic_clusters(binary_ratings_matrix, n_clusters=n_clusters, n_attrs=n_attrs, max_iter=max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating clusters...\n",
      "(943, 943)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rsenv2/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 1000 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated clusters.\n"
     ]
    }
   ],
   "source": [
    "# Create user clusters based off how often they interacted with the same item\n",
    "print('Calculating clusters...')\n",
    "co_occurence_matrix = binary_ratings_matrix @ binary_ratings_matrix.T\n",
    "print(co_occurence_matrix.shape)\n",
    "\n",
    "# Matrix factorize co_occurence_matrix to get embeddings\n",
    "nmf_cooc = NMF(n_components=n_attrs, max_iter=max_iter)\n",
    "W_topics = nmf_cooc.fit_transform(co_occurence_matrix)\n",
    "\n",
    "# cluster W_topics\n",
    "cluster_ids = KMeans(n_clusters=n_clusters, max_iter=max_iter, random_state=random_state).fit_predict(W_topics)\n",
    "\n",
    "print('Calculated clusters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fn = ''\n",
    "probabilistic = False\n",
    "globals.ALPHA = 0.2\n",
    "alpha = globals.ALPHA\n",
    "\n",
    "# User parameters\n",
    "drift = 0.2\n",
    "attention_exp=1.5\n",
    "pair_all=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 20\n",
      "Number of users: 943\n",
      "Number of user pairs: 790474\n"
     ]
    }
   ],
   "source": [
    "num_users = len(user_representation)\n",
    "num_items = len(item_representation)\n",
    "print(f'Number of items: {num_items}')\n",
    "print(f'Number of users: {num_users}')\n",
    "\n",
    "if pair_all:\n",
    "    # All possible user pairs\n",
    "    user_pairs = [(u_idx, v_idx) for u_idx in range(len(user_representation)) for v_idx in range(len(user_representation))]\n",
    "\n",
    "else:\n",
    "    # create user_pairs by pairing users only with others that are not in the same cluster\n",
    "    user_pairs = []\n",
    "    for u_idx in range(num_users):\n",
    "        for v_idx in range(num_users):\n",
    "            if cluster_ids[u_idx] != cluster_ids[v_idx]:\n",
    "                user_pairs.append((u_idx, v_idx))\n",
    "print(f'Number of user pairs: {len(user_pairs)}')\n",
    "                \n",
    "users = Users(actual_user_profiles=user_representation, \n",
    "              repeat_interactions=False, \n",
    "              drift=drift,\n",
    "              attention_exp=attention_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSEMeasurement()\n",
    "measurements_list = [\n",
    "    InteractionMeasurement(), \n",
    "    mse,  \n",
    "    InteractionSpread(), \n",
    "    InteractionSimilarity(pairs=user_pairs), \n",
    "    RecSimilarity(pairs=user_pairs), \n",
    "    SerendipityMetric(), \n",
    "    DiversityMetric(), \n",
    "    NoveltyMetric(),\n",
    "    RecallMeasurement(),\n",
    "    # MeanNumberOfTopics(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "config = {\n",
    "    'actual_user_representation': users,\n",
    "    'actual_item_representation': item_representation,\n",
    "    'item_topics': item_topics,\n",
    "    'num_attributes': n_attrs,\n",
    "    'num_items_per_iter': 10,\n",
    "    'seed': 42,\n",
    "    'record_base_state': True,\n",
    "}\n",
    "\n",
    "model_name='myopic'\n",
    "requires_alpha = False\n",
    "\n",
    "if score_fn:\n",
    "    if score_fn == 'cosine_sim':\n",
    "        config['score_fn'] = cosine_sim\n",
    "        requires_alpha = True\n",
    "    elif score_fn == 'entropy':\n",
    "        config['score_fn'] = entropy\n",
    "        requires_alpha = True\n",
    "    elif score_fn == 'content_fairness':\n",
    "        config['score_fn'] = content_fairness        \n",
    "    else:\n",
    "        raise Exception('Given score function does not exist.')\n",
    "    model_name = score_fn\n",
    "\n",
    "if probabilistic:\n",
    "    config['probabilistic_recommendations'] = True\n",
    "    model_name += '_prob'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model representation of users and items are given by:\n",
      "- An all-zeros matrix of users of dimension (943, 20)\n",
      "- A randomly generated matrix of items of dimension (20, 1682)\n"
     ]
    }
   ],
   "source": [
    "model = BubbleBurster(**config)\n",
    "\n",
    "model.add_metrics(*measurements_list)\n",
    "\n",
    "print(\"Model representation of users and items are given by:\")\n",
    "print(f\"- An all-zeros matrix of users of dimension {model.predicted_user_profiles.shape}\")\n",
    "print(f\"- A randomly generated matrix of items of dimension {model.predicted_item_attributes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVSpo-18VHH4",
    "outputId": "cef8814a-7ccb-4da1-dae9-aab8fdcad51d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:37<00:00,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Fair Model\n",
    "train_timesteps=5\n",
    "model.startup_and_train(timesteps=train_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/20 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3734/3800206109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrun_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/rsenv2/lib/python3.7/site-packages/trecs/models/recommender.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, timesteps, startup, train_between_steps, random_items_per_iter, vary_random_items_per_iter, repeated_items, no_new_items, disable_tqdm)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;31m# record state and compute metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstartup_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_new_items\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rsenv2/lib/python3.7/site-packages/trecs/metrics/measurement.py\u001b[0m in \u001b[0;36mmeasure_content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \"\"\"\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fair-recommender-systems/wrapper/metrics/evaluation_metrics.py\u001b[0m in \u001b[0;36mmeasure\u001b[0;34m(self, recommender)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mitem_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0ms_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicted_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_by_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mo' is not defined"
     ]
    }
   ],
   "source": [
    "run_timesteps=20\n",
    "model.run(timesteps=run_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "zbOWgfvksMs4",
    "outputId": "c770281b-05e6-4b3c-c79c-91809cd5f76a"
   },
   "outputs": [],
   "source": [
    "import src\n",
    "reload(src.utils)\n",
    "from src.utils import *\n",
    "    \n",
    "# Save measurements\n",
    "measurements_dir = f'artefacts/measurements/'\n",
    "file_name = f'{model_name}_measurements_{train_timesteps}trainTimesteps_{run_timesteps}runTimesteps_{n_attrs}nAttrs_{n_clusters}nClusters_{drift}Drift_{attention_exp}AttentionExp_{pair_all}PairAll'\n",
    "measurements_path = measurements_dir + file_name\n",
    "if requires_alpha:\n",
    "    measurements_path += f'_{alpha}Lambda'\n",
    "measurements_path += '.csv'\n",
    "measurements_df = load_or_create_measurements_df(model, model_name, train_timesteps, measurements_path)\n",
    "measurements_df.to_csv(measurements_path)\n",
    "print('Measurements saved.')\n",
    "\n",
    "# Create df for parametrics\n",
    "numeric_cols = ['trainTimesteps', 'runTimesteps', 'nAttrs', 'nClusters', 'Lambda']\n",
    "columns = ['model_name'] + numeric_cols\n",
    "\n",
    "data = [[model_name, train_timesteps, run_timesteps, n_attrs, n_clusters, None]]\n",
    "if requires_alpha:\n",
    "    data = [[model_name, train_timesteps, run_timesteps, n_attrs, n_clusters, alpha]]\n",
    "\n",
    "parameters_df = pd.DataFrame(data,\n",
    "                             columns = columns)\n",
    "for col in numeric_cols:\n",
    "    parameters_df[col] = pd.to_numeric(parameters_df[col])\n",
    "    \n",
    "plot_measurements([measurements_df], parameters_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novelty that is based on probability calculation from generate_recommendations \n",
    "# (leave previously interacted items out)\n",
    "item_counts = recommender.item_count\n",
    "item_counts[item_counts == 0] = 1\n",
    "items_self_info = (-1) * np.log(item_counts)\n",
    "\n",
    "# turn scores in probability distribution over items to ensure that all independent of the ranking function, the metric yields comparable values\n",
    "scores = recommender.predicted_scores.value\n",
    "probabilities = scores / np.sum(scores, axis=1)[:, np.newaxis]\n",
    "\n",
    "item_indices = recommender.indices\n",
    "if not recommender.users.repeat_interactions:\n",
    "    # for each user, eliminate items that have been interacted with\n",
    "    item_indices = item_indices[np.where(item_indices >= 0)]\n",
    "    item_indices = item_indices.reshape((recommender.num_users, -1))\n",
    "\n",
    "s_filtered = mo.to_dense(recommender.predicted_scores.filter_by_index(item_indices))\n",
    "row = np.repeat(recommender.users.user_vector, item_indices.shape[1])\n",
    "row = row.reshape((recommender.num_users, -1))\n",
    "permutation = s_filtered.argsort()\n",
    "rec = item_indices[row, permutation]\n",
    "# the recommended items will not be exactly determined by\n",
    "# predicted score; instead, we will sample from the sorted list\n",
    "# such that higher-preference items get more probability mass\n",
    "num_items_unseen = rec.shape[1]  # number of items unseen per user\n",
    "probabilities = np.logspace(0.0, num_items_unseen / 10.0, num=num_items_unseen, base=2)\n",
    "probabilities = probabilities / probabilities.sum()\n",
    "\n",
    "# get utility of each item given a state of users\n",
    "item_states = np.mean(probabilities, axis=0)\n",
    "\n",
    "# calculate novelty per item by multiplying self information and utility value\n",
    "item_novelties = items_self_info * item_states\n",
    "# form sum over all possible items/actions\n",
    "item_novelty = np.sum(item_novelties)\n",
    "\n",
    "item_novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_fairness(predicted_user_profiles, predicted_item_attributes):\n",
    "    \"\"\"\n",
    "    A score function that ensures that no topic is overrepresented. \n",
    "    1. Scores are predicted using the inner product (myopic).\n",
    "    2. Probabilities are created based on these scores to ensure common value range\n",
    "    3. The weight of an item to a topic is determined based on its relative value in its embeddings\n",
    "    4. A slate is created based on myopic scores.\n",
    "    5. If adding a new item to the slate would exceed the upper bound in a topic dimension, the item is not added.\n",
    "       Instead, the function proceeds to the next available item until one is found that does not exceed any weights\n",
    "       in the topic dimensions.\n",
    "    6. To ensure that the items appear in the top-k slate picked by t-recs, the scores of the slate items are\n",
    "       manually increased to be in the top-k.\n",
    "    \n",
    "    Caveats: Upper_bound and slate_size are set manually because default t-recs implementation does not allow\n",
    "    passing additional parameters to a score function. The current implementation only works with a top-k slate (non-probabilistic). Also scores are manually altered which reduces interpretability of the scores.\n",
    "    \n",
    "    Based on \"Controlling Polarization in Personalization: An Algorithmic Framework\" by Celis et. al.\n",
    "    \n",
    "    Inputs:\n",
    "        - predicted_user_profiles\n",
    "        - pretedicted_item_attributes\n",
    "    Outputs:\n",
    "        - predicted_scores_reranked (Shape: U x I)\n",
    "    \"\"\"\n",
    "    slate_size = 10\n",
    "    upper_bound = 0.75\n",
    "\n",
    "    # 1. Calculate myopic scores\n",
    "    predicted_scores =  mo.inner_product(predicted_user_profiles, predicted_item_attributes)\n",
    "\n",
    "    # 2. Calculate probabilities and sort them\n",
    "    if np.sum(predicted_scores) == 0:\n",
    "        return predicted_scores\n",
    "    probs = (predicted_scores.T / np.sum(predicted_scores, axis=1)).T\n",
    "    probs_sorted = np.flip(np.argsort(probs, axis=1), axis=1)\n",
    "    \n",
    "    # 3. Determine weight per embedding dimension for each item (scaled to [0,1])\n",
    "    gw = predicted_item_attributes.T / np.sum(predicted_item_attributes.T, axis=1)[:, np.newaxis]\n",
    "\n",
    "    num_user = len(predicted_user_profiles)\n",
    "    recs = np.empty((num_user, slate_size), dtype=int)\n",
    "    \n",
    "    for user in range(len(probs_sorted)):\n",
    "        agg_weight_per_cluster = np.zeros((len(gw[0]))) # matrix to keep track of weights of slate \n",
    "        \n",
    "        i = 0\n",
    "        # 4. Create slate in order of myopic scores\n",
    "        for item in probs_sorted[user]:\n",
    "            weight_item = gw[item]\n",
    "            # print(f'{item}: {weight_item}')\n",
    "            \n",
    "            # 5. Calculate weights if item was added to slate. If exceeds upper_bound go to next item.\n",
    "            proposed_weights = weight_item + agg_weight_per_cluster\n",
    "            if (proposed_weights <= upper_bound).all() and i < slate_size:\n",
    "                agg_weight_per_cluster = proposed_weights\n",
    "                recs[user, i] = item\n",
    "                i += 1\n",
    "\n",
    "    # 6. Manually assign new scores to items in slate\n",
    "    predicted_scores_reranked = np.copy(predicted_scores)\n",
    "    for i, user in enumerate(recs):\n",
    "        for item in np.flip(user):\n",
    "            predicted_scores_reranked[int(i), int(item)] = np.max(predicted_scores_reranked[int(i)]) + 1\n",
    "\n",
    "    return predicted_scores_reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_fn == content_fairness"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "rsenv2",
   "language": "python",
   "name": "rsenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c64b5f0e204e2b557bfa409cf6c0678c4f1eefdf2bd314df48e0edc57cda315"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
