{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing BubbleBurster instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_file.py\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../t-recs/')\n",
    "from trecs.models import ContentFiltering\n",
    "from trecs.metrics import *\n",
    "from trecs.random import Generator\n",
    "from trecs.components import Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "random_state = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_csv('../ml-100k/u.data', \n",
    "    sep=\"\\t\", \n",
    "    names=['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "movies_df = pd.read_csv('../ml-100k/u.item', sep=\"|\", names=movie_cols, encoding='latin')\n",
    "\n",
    "# display(movies_df.head(2))\n",
    "# print(movies_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_topic_clusters(binary_ratings_matrix, n_attrs:int=100, nmf_solver:str=\"mu\"):\n",
    "    \"\"\"\n",
    "    Creates clusters of movies based on their genre.\n",
    "    Inputs:\n",
    "        binary_ratings_matrix: a binary matrix of users and movies\n",
    "        n_attrs: number of attributes to use in NMF\n",
    "        nmf_solver: solver to use in NMF\n",
    "    Outputs:\n",
    "        clusters: a list of cluster assignments\n",
    "    \"\"\"\n",
    "    # Create topic clusters\n",
    "    #create co-occurence matrix from binary_interaction_matrix\n",
    "    co_occurence_matrix = binary_ratings_matrix.T @ binary_ratings_matrix\n",
    "    co_occurence_matrix\n",
    "\n",
    "    # Matrix factorize co_occurence_matrix to get embeddings\n",
    "    nmf_cooc = NMF(n_components=n_attrs, solver=nmf_solver)\n",
    "    W_topics = nmf_cooc.fit_transform(co_occurence_matrix)\n",
    "\n",
    "    # cluster W_topics\n",
    "    kmeans = KMeans(n_clusters=100, random_state=random_state).fit(W_topics)\n",
    "\n",
    "    # assign nearest cluster to observation\n",
    "    cluster_ids = kmeans.predict(W_topics)\n",
    "\n",
    "    return cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "binary_ratings_df = ratings_df.drop(columns=['Timestamp'])\n",
    "binary_ratings_df.loc[binary_ratings_df['Rating'] > 0, 'Rating'] = 1\n",
    "\n",
    "# turn dataframe into matrix where each movie is a column and each user is a row\n",
    "binary_ratings_matrix = binary_ratings_df.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madisonthantu/miniforge3/envs/fairRS/lib/python3.8/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lightfm.cross_validation import random_train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "# split data into train and test sets\n",
    "train_interactions, test_interactions = random_train_test_split(sparse.csr_matrix(binary_ratings_matrix), test_percentage=0.2, random_state=random_state)\n",
    "train_interactions = train_interactions.toarray()\n",
    "test_interactions = test_interactions.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 100) (100, 1682)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madisonthantu/miniforge3/envs/fairRS/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "n_attrs=100\n",
    "nmf = NMF(n_components=n_attrs, solver=\"mu\")\n",
    "user_representation = nmf.fit_transform(binary_ratings_matrix)\n",
    "item_representation = nmf.components_\n",
    "print(user_representation.shape, item_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = Users(size=(943,100), repeat_interactions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 100)\n",
      "(100, 1682)\n"
     ]
    }
   ],
   "source": [
    "print(user_representation.shape)\n",
    "print(item_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.models.bubble import BubbleBurster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = None\n",
    "item_topics = get_topic_clusters(binary_ratings_matrix, n_attrs=n_attrs, nmf_solver=\"mu\")\n",
    "user_topic_history = None\n",
    "item_count = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recsys.num_topics)\n",
    "print(recsys.item_topics.shape)\n",
    "print(recsys.user_topic_history.shape)\n",
    "print(np.unique(recsys.user_topic_history))\n",
    "print(recsys.item_count.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys.add_metrics(MSEMeasurement(), InteractionSpread(), AverageFeatureScoreRange())\n",
    "print(\"These are the current metrics:\")\n",
    "print(recsys.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we run the model\n",
    "recsys.run(timesteps=1)\n",
    "measurements = recsys.get_measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> Model successfully runs for 1 timestep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = recsys.__dict__.keys()\n",
    "vals = recsys.__dict__.values()\n",
    "\n",
    "for k, v in zip(keys,vals):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_update(recommender, item_count, user_topic_history, item_topics):\n",
    "    items_shown = recommender.items_shown\n",
    "    for i in range(items_shown.shape[0]):\n",
    "        items_shown_val, items_shown_count = np.unique(items_shown[i,:], return_counts=True)\n",
    "        item_count[0, items_shown_val] += 1\n",
    "        topics_shown = item_topics[items_shown_val]\n",
    "        topics_shown_val, topics_shown_count = np.unique(topics_shown, return_counts=True)\n",
    "        user_topic_history[i, topics_shown_val] += topics_shown_count\n",
    "        if (sum(items_shown_count) != 10):\n",
    "            print(\"DUPLICATE ITEMS IN SLATE\", items_shown_count)\n",
    "            break\n",
    "    return item_count, user_topic_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_count = np.zeros((1,recsys.num_items))\n",
    "test_user_topic_history = np.zeros((recsys.num_users, recsys.num_topics))\n",
    "\n",
    "test_item_count, test_user_topic_history = state_update(recsys, test_item_count, test_user_topic_history, item_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(recsys.item_count, test_item_count))\n",
    "print(np.array_equal(recsys.user_topic_history, test_user_topic_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> Two values are equal after 1 iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we run the model\n",
    "recsys.run(timesteps=1)\n",
    "measurements = recsys.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_count, test_user_topic_history = state_update(recsys, test_item_count, test_user_topic_history, item_topics)\n",
    "print(np.array_equal(recsys.item_count, test_item_count))\n",
    "print(np.array_equal(recsys.user_topic_history, test_user_topic_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> The two values are equal after a second iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurements.keys():\n",
    "    print(key, measurements[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.add_metrics(MSEMeasurement(), InteractionSpread(), AverageFeatureScoreRange())\n",
    "print(\"These are the current metrics:\")\n",
    "print(recsys.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.startup_and_train(timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `NoveltyMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import NoveltyMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), NoveltyMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = bubble.users.actual_user_scores.filter_by_index(bubble.items_shown).reshape(bubble.num_users, bubble.num_items_per_iter)#[:5,:10]#.shape\n",
    "# y = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "# np.array_equal(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = bubble.users.get_actual_user_scores().get_item_scores(bubble.items_shown)\n",
    "# # t.filter_by_index(bubble.items_shown)\n",
    "# t[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `SerendipityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the current metrics:\n",
      "[<trecs.metrics.measurement.MSEMeasurement object at 0x14001c070>, <wrapper.metrics.evalmetrics.SerendipityMetric object at 0x147e44790>]\n"
     ]
    }
   ],
   "source": [
    "from wrapper.metrics import SerendipityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), SerendipityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.40s/it]\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse [0.22497117411703932, 0.17329999049211842]\n",
      "serendipity_metric [None, 0.43966065747613997]\n",
      "timesteps [0 1]\n"
     ]
    }
   ],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 10)\n",
      "(943, 1682)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "0.43966065747613997\n"
     ]
    }
   ],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is serendipity equal?  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.48s/it]\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 10)\n",
      "(943, 1682)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "0.5808059384941675\n"
     ]
    }
   ],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is serendipity equal?  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.49s/it]\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 10)\n",
      "(943, 1682)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "(943, 10)\n",
      "0.5467656415694592\n"
     ]
    }
   ],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is serendipity equal?  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse [0.22497117411703932, 0.17329999049211842, 0.1686162924531914, 0.16546895480023838]\n",
      "serendipity_metric [None, 0.43966065747613997, 0.5808059384941675, 0.5467656415694592]\n",
      "timesteps [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_topics.size == np.count_nonzero((new_topics==0) | (new_topics==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82ad1e6da8c8551612185ff57ab4e881be31b0c67a550f3cbdb2f98515f5914e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('fairRS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
