{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Contents\n",
    "\n",
    "1. ### Testing `BubbleBurster` instantiation\n",
    "2. ### Testing evalmetrics - `NoveltyMetric`\n",
    "3. ### Testing evalmetrics - `SerendipityMetric`\n",
    "4. ### Testing evalmetrics - `DiversityMetric`\n",
    "5. ### Testing **all** evalmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_file.py\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../t-recs/')\n",
    "from trecs.models import ContentFiltering\n",
    "from trecs.metrics import *\n",
    "from trecs.random import Generator\n",
    "from trecs.components import Users\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "random_state = np.random.seed(42)\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "ratings_df = pd.read_csv('../ml-100k/u.data', \n",
    "    sep=\"\\t\", \n",
    "    names=['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    ")\n",
    "\n",
    "movie_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "movies_df = pd.read_csv('../ml-100k/u.item', sep=\"|\", names=movie_cols, encoding='latin')\n",
    "\n",
    "# display(movies_df.head(2))\n",
    "# print(movies_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_topic_clusters(binary_ratings_matrix, n_attrs:int=100, nmf_solver:str=\"mu\"):\n",
    "    \"\"\"\n",
    "    Creates clusters of movies based on their genre.\n",
    "    Inputs:\n",
    "        binary_ratings_matrix: a binary matrix of users and movies\n",
    "        n_attrs: number of attributes to use in NMF\n",
    "        nmf_solver: solver to use in NMF\n",
    "    Outputs:\n",
    "        clusters: a list of cluster assignments\n",
    "    \"\"\"\n",
    "    # Create topic clusters\n",
    "    #create co-occurence matrix from binary_interaction_matrix\n",
    "    co_occurence_matrix = binary_ratings_matrix.T @ binary_ratings_matrix\n",
    "    co_occurence_matrix\n",
    "\n",
    "    # Matrix factorize co_occurence_matrix to get embeddings\n",
    "    nmf_cooc = NMF(n_components=n_attrs, solver=nmf_solver)\n",
    "    W_topics = nmf_cooc.fit_transform(co_occurence_matrix)\n",
    "\n",
    "    # cluster W_topics\n",
    "    kmeans = KMeans(n_clusters=100, random_state=random_state).fit(W_topics)\n",
    "\n",
    "    # assign nearest cluster to observation\n",
    "    cluster_ids = kmeans.predict(W_topics)\n",
    "\n",
    "    return cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madisonthantu/miniforge3/envs/fairRS/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 100) (100, 1682)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "binary_ratings_df = ratings_df.drop(columns=['Timestamp'])\n",
    "binary_ratings_df.loc[binary_ratings_df['Rating'] > 0, 'Rating'] = 1\n",
    "\n",
    "# turn dataframe into matrix where each movie is a column and each user is a row\n",
    "binary_ratings_matrix = binary_ratings_df.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0).to_numpy()\n",
    "\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "# split data into train and test sets\n",
    "train_interactions, test_interactions = random_train_test_split(sparse.csr_matrix(binary_ratings_matrix), test_percentage=0.2, random_state=random_state)\n",
    "train_interactions = train_interactions.toarray()\n",
    "test_interactions = test_interactions.toarray()\n",
    "\n",
    "n_attrs=100\n",
    "nmf = NMF(n_components=n_attrs, solver=\"mu\")\n",
    "user_representation = nmf.fit_transform(binary_ratings_matrix)\n",
    "item_representation = nmf.components_\n",
    "print(user_representation.shape, item_representation.shape)\n",
    "\n",
    "num_topics = None\n",
    "item_topics = get_topic_clusters(binary_ratings_matrix, n_attrs=n_attrs, nmf_solver=\"mu\")\n",
    "user_topic_history = None\n",
    "item_count = None\n",
    "\n",
    "from wrapper.models.bubble import BubbleBurster\n",
    "\n",
    "users = Users(size=(943,100), repeat_interactions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `BubbleBurster` model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recsys.num_topics)\n",
    "print(recsys.item_topics.shape)\n",
    "print(recsys.user_topic_history.shape)\n",
    "print(np.unique(recsys.user_topic_history))\n",
    "print(recsys.item_count.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys.add_metrics(MSEMeasurement(), InteractionSpread(), AverageFeatureScoreRange())\n",
    "print(\"These are the current metrics:\")\n",
    "print(recsys.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we run the model\n",
    "recsys.run(timesteps=1)\n",
    "measurements = recsys.get_measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> Model successfully runs for 1 timestep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = recsys.__dict__.keys()\n",
    "vals = recsys.__dict__.values()\n",
    "\n",
    "for k, v in zip(keys,vals):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_update(recommender, item_count, user_topic_history, item_topics):\n",
    "    items_shown = recommender.items_shown\n",
    "    for i in range(items_shown.shape[0]):\n",
    "        items_shown_val, items_shown_count = np.unique(items_shown[i,:], return_counts=True)\n",
    "        item_count[0, items_shown_val] += 1\n",
    "        topics_shown = item_topics[items_shown_val]\n",
    "        topics_shown_val, topics_shown_count = np.unique(topics_shown, return_counts=True)\n",
    "        user_topic_history[i, topics_shown_val] += topics_shown_count\n",
    "        if (sum(items_shown_count) != 10):\n",
    "            print(\"DUPLICATE ITEMS IN SLATE\", items_shown_count)\n",
    "            break\n",
    "    return item_count, user_topic_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_count = np.zeros((1,recsys.num_items))\n",
    "test_user_topic_history = np.zeros((recsys.num_users, recsys.num_topics))\n",
    "\n",
    "test_item_count, test_user_topic_history = state_update(recsys, test_item_count, test_user_topic_history, item_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(recsys.item_count, test_item_count))\n",
    "print(np.array_equal(recsys.user_topic_history, test_user_topic_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> Two values are equal after 1 iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we run the model\n",
    "recsys.run(timesteps=1)\n",
    "measurements = recsys.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_count, test_user_topic_history = state_update(recsys, test_item_count, test_user_topic_history, item_topics)\n",
    "print(np.array_equal(recsys.item_count, test_item_count))\n",
    "print(np.array_equal(recsys.user_topic_history, test_user_topic_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> The two values are equal after a second iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurements.keys():\n",
    "    print(key, measurements[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.add_metrics(MSEMeasurement(), InteractionSpread(), AverageFeatureScoreRange())\n",
    "print(\"These are the current metrics:\")\n",
    "print(recsys.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.startup_and_train(timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `NoveltyMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import NoveltyMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), NoveltyMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = bubble.users.actual_user_scores.filter_by_index(bubble.items_shown).reshape(bubble.num_users, bubble.num_items_per_iter)#[:5,:10]#.shape\n",
    "# y = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "# np.array_equal(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = bubble.users.get_actual_user_scores().get_item_scores(bubble.items_shown)\n",
    "# # t.filter_by_index(bubble.items_shown)\n",
    "# t[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `SerendipityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import SerendipityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), SerendipityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics.size == np.count_nonzero((new_topics==0) | (new_topics==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `DiversityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    # actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the current metrics:\n",
      "[<trecs.metrics.measurement.MSEMeasurement object at 0x13f580730>, <wrapper.metrics.evaluation_metrics.DiversityMetric object at 0x13f5a9100>]\n"
     ]
    }
   ],
   "source": [
    "from wrapper.metrics import DiversityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), DiversityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.10s/it]\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse [0.21112593798424928, 0.15463305960391496]\n",
      "diversity_metric [None, 0.9593024625898433]\n",
      "timesteps [0 1]\n"
     ]
    }
   ],
   "source": [
    "for key in measurements.keys():\n",
    "    print(key, measurements[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9593024625898433\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Getting all possible 2-item combinations (the indices) for the items in a slate\n",
    "combos = combinations(np.arange(bubble.num_items_per_iter), 2)\n",
    "items_shown = bubble.items_shown\n",
    "\n",
    "topic_similarity = np.zeros(bubble.num_users)\n",
    "\n",
    "stop = 0\n",
    "for i in combos:\n",
    "    # topic_similarity is equal to the number of 2-item combinations in which the items' topics are the same\n",
    "    item_pair = items_shown[:, i]\n",
    "    topic_pair = bubble.item_topics[item_pair]\n",
    "    topic_similarity += (topic_pair[:,0] == topic_pair[:,1])\n",
    "\n",
    "slate_diversity = 1 - ((1 / (bubble.num_items_per_iter * (bubble.num_items_per_iter-1))) * topic_similarity)\n",
    "diversity = np.mean(slate_diversity)\n",
    "print(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diversity equal?  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is diversity equal? \", measurements['diversity_metric'][-1] == diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  4.00s/it]\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9367856722045481\n"
     ]
    }
   ],
   "source": [
    "# Getting all possible 2-item combinations (the indices) for the items in a slate\n",
    "combos = combinations(np.arange(bubble.num_items_per_iter), 2)\n",
    "items_shown = bubble.items_shown\n",
    "\n",
    "topic_similarity = np.zeros(bubble.num_users)\n",
    "\n",
    "stop = 0\n",
    "for i in combos:\n",
    "    # topic_similarity is equal to the number of 2-item combinations in which the items' topics are the same\n",
    "    item_pair = items_shown[:, i]\n",
    "    topic_pair = bubble.item_topics[item_pair]\n",
    "    topic_similarity += (topic_pair[:,0] == topic_pair[:,1])\n",
    "\n",
    "slate_diversity = 1 - ((1 / (bubble.num_items_per_iter * (bubble.num_items_per_iter-1))) * topic_similarity)\n",
    "diversity = np.mean(slate_diversity)\n",
    "print(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diversity equal?  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is diversity equal? \", measurements['diversity_metric'][-1] == diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9379992930364087\n"
     ]
    }
   ],
   "source": [
    "# Getting all possible 2-item combinations (the indices) for the items in a slate\n",
    "combos = combinations(np.arange(bubble.num_items_per_iter), 2)\n",
    "items_shown = bubble.items_shown\n",
    "\n",
    "topic_similarity = np.zeros(bubble.num_users)\n",
    "\n",
    "stop = 0\n",
    "for i in combos:\n",
    "    # topic_similarity is equal to the number of 2-item combinations in which the items' topics are the same\n",
    "    item_pair = items_shown[:, i]\n",
    "    topic_pair = bubble.item_topics[item_pair]\n",
    "    topic_similarity += (topic_pair[:,0] == topic_pair[:,1])\n",
    "\n",
    "slate_diversity = 1 - ((1 / (bubble.num_items_per_iter * (bubble.num_items_per_iter-1))) * topic_similarity)\n",
    "diversity = np.mean(slate_diversity)\n",
    "print(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is diversity equal?  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is diversity equal? \", measurements['diversity_metric'][-1] == diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse [0.21112593798424928, 0.15463305960391496, 0.15069895833495228, 0.14884573548635277]\n",
      "diversity_metric [None, 0.9593024625898433, 0.9367856722045481, 0.9379992930364087]\n",
      "timesteps [0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing **all** evalmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeates **not** allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the current metrics:\n",
      "[<trecs.metrics.measurement.MSEMeasurement object at 0x13f9c8eb0>, <wrapper.metrics.evaluation_metrics.NoveltyMetric object at 0x13f6ea3a0>, <wrapper.metrics.evaluation_metrics.SerendipityMetric object at 0x13f68fbb0>, <wrapper.metrics.evaluation_metrics.DiversityMetric object at 0x13f9c3f70>]\n"
     ]
    }
   ],
   "source": [
    "from wrapper.metrics import NoveltyMetric, SerendipityMetric, DiversityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), NoveltyMetric(), SerendipityMetric(), DiversityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse - [0.1784592287533019, 0.15972060105745922]\n",
      "novelty_metric - [None, 0.10359330690544968]\n",
      "serendipity_metric - [None, 0.43573700954400846]\n",
      "diversity_metric - [None, 0.9374337221633087]\n",
      "timesteps - [0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()\n",
    "\n",
    "for key in measurements:\n",
    "    print(key, \"-\", measurements[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:39<00:00,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements for iters:  [  0  10  20  30  40  50  60  70  80  90 100]\n",
      "mse \n",
      "\t [0.15133805046835788, 0.14467154413681826, 0.1487754675347435, 0.14968479467438453, 0.15009435522751075, 0.15031112485836184, 0.15043348735603654, 0.1504960683452621, 0.1505500545564818, 0.15058893320125294]\n",
      "novelty_metric \n",
      "\t [5.884708427563204, 0.9266455087996867, -1.770680279581783, -3.252054794580073, -4.284946453899219, -5.087026737524778, -5.772054798440263, -6.3372387227837885, -6.828237437668015, -7.241519610795977]\n",
      "serendipity_metric \n",
      "\t [0.5580063626723224, 0.1391304347826087, 0.00752916224814422, 0.0030752916224814422, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "diversity_metric \n",
      "\t [0.9325556733828209, 0.9256981265464829, 0.930965005302227, 0.9309296571226583, 0.9318604925179688, 0.9322493224932249, 0.9322375397667022, 0.9324849770236833, 0.9325203252032521, 0.9325556733828209]\n",
      "timesteps \n",
      "\t [ 1 11 21 31 41 51 61 71 81 91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=100)\n",
    "measurements = bubble.get_measurements()\n",
    "\n",
    "# print(\"Measurements for iters: \", str(np.arange(11) * 10))\n",
    "for key in measurements:\n",
    "    print(key, \"\\n\\t\", measurements[key][1::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeates **allowed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    # actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the current metrics:\n",
      "[<trecs.metrics.measurement.MSEMeasurement object at 0x13f9c3580>, <wrapper.metrics.evaluation_metrics.NoveltyMetric object at 0x13f9c3670>, <wrapper.metrics.evaluation_metrics.SerendipityMetric object at 0x13f9c39d0>, <wrapper.metrics.evaluation_metrics.DiversityMetric object at 0x13f6eac10>]\n"
     ]
    }
   ],
   "source": [
    "from wrapper.metrics import NoveltyMetric, SerendipityMetric, DiversityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), NoveltyMetric(), SerendipityMetric(), DiversityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [08:59<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse \n",
      "\t [0.16016304470609172, 0.15044847206595782, 0.14997569901227137, 0.1498756583026104, 0.14980545679817098, 0.14978220692505106, 0.1497653550290884, 0.14975330127143666, 0.14975122391021908, 0.14974712464598763]\n",
      "novelty_metric \n",
      "\t [-0.059545925041144414, 0.5324088522042537, -1.7418368008451115, -3.1167789215026644, -4.084608875659482, -4.842937001404294, -5.4686423533838155, -6.00312933764717, -6.461690573721362, -6.87981383205485]\n",
      "serendipity_metric \n",
      "\t [0.4610816542948038, 0.17083775185577943, 0.03054082714740191, 0.007635206786850477, 0.040402969247083774, 0.010286320254506893, 0.02608695652173913, 0.019300106044538707, 0.015164369034994699, 0.000848356309650053]\n",
      "diversity_metric \n",
      "\t [0.9327206315541418, 0.9276658418758101, 0.9281135854836811, 0.9280664545775893, 0.9278661482266998, 0.9279604100388832, 0.9279957582184519, 0.9279250618593143, 0.9278897136797455, 0.9277954518675623]\n",
      "timesteps \n",
      "\t [ 1 11 21 31 41 51 61 71 81 91]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bubble.run(timesteps=100)\n",
    "measurements = bubble.get_measurements()\n",
    "\n",
    "# print(\"Measurements for iters: \", str(np.arange(11) * 10))\n",
    "for key in measurements:\n",
    "    print(key, \"\\n\\t\", measurements[key][1::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82ad1e6da8c8551612185ff57ab4e881be31b0c67a550f3cbdb2f98515f5914e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.15 ('fairRS')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
