{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Contents\n",
    "\n",
    "1. ### Testing `BubbleBurster` instantiation\n",
    "2. ### Testing evalmetrics - `NoveltyMetric`\n",
    "3. ### Testing evalmetrics - `SerendipityMetric`\n",
    "4. ### Testing evalmetrics - `DiversityMetric`\n",
    "5. ### Testing **all** evalmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../ml-100k/u.data does not exist: '../ml-100k/u.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4436\\2611199447.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m ratings_df = pd.read_csv('../ml-100k/u.data', \n\u001b[0;32m     23\u001b[0m     \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'UserID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MovieID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jannik\\anaconda3\\envs\\rsenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jannik\\anaconda3\\envs\\rsenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jannik\\anaconda3\\envs\\rsenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jannik\\anaconda3\\envs\\rsenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Jannik\\anaconda3\\envs\\rsenv\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ../ml-100k/u.data does not exist: '../ml-100k/u.data'"
     ]
    }
   ],
   "source": [
    "# some_file.py\n",
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, '../t-recs/')\n",
    "from trecs.models import ContentFiltering\n",
    "from trecs.metrics import *\n",
    "from trecs.random import Generator\n",
    "from trecs.components import Users\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "random_state = np.random.seed(42)\n",
    "\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "ratings_df = pd.read_csv('../data/ml-100k/u.data', \n",
    "    sep=\"\\t\", \n",
    "    names=['UserID', 'MovieID', 'Rating', 'Timestamp']\n",
    ")\n",
    "\n",
    "movie_cols = ['movie_id', 'title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "movies_df = pd.read_csv('../ml-100k/u.item', sep=\"|\", names=movie_cols, encoding='latin')\n",
    "\n",
    "# display(movies_df.head(2))\n",
    "# print(movies_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def get_topic_clusters(binary_ratings_matrix, n_attrs:int=100, nmf_solver:str=\"mu\"):\n",
    "    \"\"\"\n",
    "    Creates clusters of movies based on their genre.\n",
    "    Inputs:\n",
    "        binary_ratings_matrix: a binary matrix of users and movies\n",
    "        n_attrs: number of attributes to use in NMF\n",
    "        nmf_solver: solver to use in NMF\n",
    "    Outputs:\n",
    "        clusters: a list of cluster assignments\n",
    "    \"\"\"\n",
    "    # Create topic clusters\n",
    "    #create co-occurence matrix from binary_interaction_matrix\n",
    "    co_occurence_matrix = binary_ratings_matrix.T @ binary_ratings_matrix\n",
    "    co_occurence_matrix\n",
    "\n",
    "    # Matrix factorize co_occurence_matrix to get embeddings\n",
    "    nmf_cooc = NMF(n_components=n_attrs, solver=nmf_solver)\n",
    "    W_topics = nmf_cooc.fit_transform(co_occurence_matrix)\n",
    "\n",
    "    # cluster W_topics\n",
    "    kmeans = KMeans(n_clusters=100, random_state=random_state).fit(W_topics)\n",
    "\n",
    "    # assign nearest cluster to observation\n",
    "    cluster_ids = kmeans.predict(W_topics)\n",
    "\n",
    "    return cluster_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "binary_ratings_df = ratings_df.drop(columns=['Timestamp'])\n",
    "binary_ratings_df.loc[binary_ratings_df['Rating'] > 0, 'Rating'] = 1\n",
    "\n",
    "# turn dataframe into matrix where each movie is a column and each user is a row\n",
    "binary_ratings_matrix = binary_ratings_df.pivot(index='UserID', columns='MovieID', values='Rating').fillna(0).to_numpy()\n",
    "\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "# split data into train and test sets\n",
    "train_interactions, test_interactions = random_train_test_split(sparse.csr_matrix(binary_ratings_matrix), test_percentage=0.2, random_state=random_state)\n",
    "train_interactions = train_interactions.toarray()\n",
    "test_interactions = test_interactions.toarray()\n",
    "\n",
    "n_attrs=100\n",
    "nmf = NMF(n_components=n_attrs, solver=\"mu\")\n",
    "user_representation = nmf.fit_transform(binary_ratings_matrix)\n",
    "item_representation = nmf.components_\n",
    "print(user_representation.shape, item_representation.shape)\n",
    "\n",
    "num_topics = None\n",
    "item_topics = get_topic_clusters(binary_ratings_matrix, n_attrs=n_attrs, nmf_solver=\"mu\")\n",
    "user_topic_history = None\n",
    "item_count = None\n",
    "\n",
    "from wrapper.models.bubble import BubbleBurster\n",
    "\n",
    "users = Users(size=(943,100), repeat_interactions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing `BubbleBurster` model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recsys.num_topics)\n",
    "print(recsys.item_topics.shape)\n",
    "print(recsys.user_topic_history.shape)\n",
    "print(np.unique(recsys.user_topic_history))\n",
    "print(recsys.item_count.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recsys.add_metrics(MSEMeasurement(), InteractionSpread(), AverageFeatureScoreRange())\n",
    "print(\"These are the current metrics:\")\n",
    "print(recsys.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we run the model\n",
    "recsys.run(timesteps=1)\n",
    "measurements = recsys.get_measurements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> Model successfully runs for 1 timestep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = recsys.__dict__.keys()\n",
    "vals = recsys.__dict__.values()\n",
    "\n",
    "for k, v in zip(keys,vals):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_update(recommender, item_count, user_topic_history, item_topics):\n",
    "    items_shown = recommender.items_shown\n",
    "    for i in range(items_shown.shape[0]):\n",
    "        items_shown_val, items_shown_count = np.unique(items_shown[i,:], return_counts=True)\n",
    "        item_count[0, items_shown_val] += 1\n",
    "        topics_shown = item_topics[items_shown_val]\n",
    "        topics_shown_val, topics_shown_count = np.unique(topics_shown, return_counts=True)\n",
    "        user_topic_history[i, topics_shown_val] += topics_shown_count\n",
    "        if (sum(items_shown_count) != 10):\n",
    "            print(\"DUPLICATE ITEMS IN SLATE\", items_shown_count)\n",
    "            break\n",
    "    return item_count, user_topic_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_count = np.zeros((1,recsys.num_items))\n",
    "test_user_topic_history = np.zeros((recsys.num_users, recsys.num_topics))\n",
    "\n",
    "test_item_count, test_user_topic_history = state_update(recsys, test_item_count, test_user_topic_history, item_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array_equal(recsys.item_count, test_item_count))\n",
    "print(np.array_equal(recsys.user_topic_history, test_user_topic_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> Two values are equal after 1 iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we run the model\n",
    "recsys.run(timesteps=1)\n",
    "measurements = recsys.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_item_count, test_user_topic_history = state_update(recsys, test_item_count, test_user_topic_history, item_topics)\n",
    "print(np.array_equal(recsys.item_count, test_item_count))\n",
    "print(np.array_equal(recsys.user_topic_history, test_user_topic_history))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**-> The two values are equal after a second iteration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurements.keys():\n",
    "    print(key, measurements[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.add_metrics(MSEMeasurement(), InteractionSpread(), AverageFeatureScoreRange())\n",
    "print(\"These are the current metrics:\")\n",
    "print(recsys.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.startup_and_train(timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `NoveltyMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import NoveltyMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), NoveltyMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = bubble.users.actual_user_scores.filter_by_index(bubble.items_shown).reshape(bubble.num_users, bubble.num_items_per_iter)#[:5,:10]#.shape\n",
    "# y = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "# np.array_equal(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = bubble.users.get_actual_user_scores().get_item_scores(bubble.items_shown)\n",
    "# # t.filter_by_index(bubble.items_shown)\n",
    "# t[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slate_items_self_info = bubble.item_count[bubble.items_shown]\n",
    "slate_items_self_info = (-1) * np.log(np.divide(slate_items_self_info, bubble.num_users))\n",
    "print(slate_items_self_info.shape)\n",
    "\n",
    "slate_items_pred_score = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1)#.shape\n",
    "print(slate_items_pred_score.shape)\n",
    "\n",
    "slate_novelty = np.multiply(slate_items_self_info, slate_items_pred_score)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "slate_novelty = np.sum(slate_novelty, axis=1)\n",
    "print(slate_novelty.shape)\n",
    "\n",
    "novelty = np.mean(slate_novelty)\n",
    "print(novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is novelty equal? \", measurements['novelty_metric'][-1] == novelty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `SerendipityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import SerendipityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), SerendipityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices for the items shown\n",
    "items_shown = bubble.items_shown\n",
    "print(items_shown.shape)\n",
    "# Scores for the items shown\n",
    "user_scores = bubble.users.actual_user_scores.value\n",
    "print(user_scores.shape)\n",
    "# Scores for just the shown items that have a score greater than 0\n",
    "user_scores_items_shown = np.take_along_axis(bubble.users.actual_user_scores.value, bubble.items_shown, axis=1) > 0\n",
    "print(user_scores_items_shown.shape)\n",
    "# Topics that correspond to each item shown\n",
    "topics_shown = bubble.item_topics[bubble.items_shown]\n",
    "print(topics_shown.shape)\n",
    "\"\"\"\n",
    "Need to update the below 2 lines depending on how user_topic_history is implemented in the wrapper class\n",
    "\"\"\"\n",
    "# Boolean matrix where value=1 if the topic shown is not in the user history, otherwise value=0\n",
    "new_topics = np.apply_along_axis(np.isin, 1, topics_shown, bubble.user_topic_history, invert=True)\n",
    "print(new_topics.shape)\n",
    "\n",
    "# # calculate serendipity for all items presented to each user\n",
    "items_shown_serendipity = np.multiply(new_topics, user_scores_items_shown)\n",
    "print(items_shown_serendipity.shape)\n",
    "# Calculate average serendipity - average serendipity by slate AND users\n",
    "serendipity = np.mean(items_shown_serendipity)#np.sum(np.multiply(new_topics, user_scores_items_shown)) / bubble.num_users\n",
    "print(serendipity)\n",
    "# # to complete the measurement, call `self.observe(metric_value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is serendipity equal? \", measurements['serendipity_metric'][-1] == serendipity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics.size == np.count_nonzero((new_topics==0) | (new_topics==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing evalmetrics - `DiversityMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    # actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import DiversityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), DiversityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in measurements.keys():\n",
    "    print(key, measurements[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Getting all possible 2-item combinations (the indices) for the items in a slate\n",
    "combos = combinations(np.arange(bubble.num_items_per_iter), 2)\n",
    "items_shown = bubble.items_shown\n",
    "\n",
    "topic_similarity = np.zeros(bubble.num_users)\n",
    "\n",
    "stop = 0\n",
    "for i in combos:\n",
    "    # topic_similarity is equal to the number of 2-item combinations in which the items' topics are the same\n",
    "    item_pair = items_shown[:, i]\n",
    "    topic_pair = bubble.item_topics[item_pair]\n",
    "    topic_similarity += (topic_pair[:,0] == topic_pair[:,1])\n",
    "\n",
    "slate_diversity = 1 - ((1 / (bubble.num_items_per_iter * (bubble.num_items_per_iter-1))) * topic_similarity)\n",
    "diversity = np.mean(slate_diversity)\n",
    "print(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is diversity equal? \", measurements['diversity_metric'][-1] == diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all possible 2-item combinations (the indices) for the items in a slate\n",
    "combos = combinations(np.arange(bubble.num_items_per_iter), 2)\n",
    "items_shown = bubble.items_shown\n",
    "\n",
    "topic_similarity = np.zeros(bubble.num_users)\n",
    "\n",
    "stop = 0\n",
    "for i in combos:\n",
    "    # topic_similarity is equal to the number of 2-item combinations in which the items' topics are the same\n",
    "    item_pair = items_shown[:, i]\n",
    "    topic_pair = bubble.item_topics[item_pair]\n",
    "    topic_similarity += (topic_pair[:,0] == topic_pair[:,1])\n",
    "\n",
    "slate_diversity = 1 - ((1 / (bubble.num_items_per_iter * (bubble.num_items_per_iter-1))) * topic_similarity)\n",
    "diversity = np.mean(slate_diversity)\n",
    "print(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is diversity equal? \", measurements['diversity_metric'][-1] == diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all possible 2-item combinations (the indices) for the items in a slate\n",
    "combos = combinations(np.arange(bubble.num_items_per_iter), 2)\n",
    "items_shown = bubble.items_shown\n",
    "\n",
    "topic_similarity = np.zeros(bubble.num_users)\n",
    "\n",
    "stop = 0\n",
    "for i in combos:\n",
    "    # topic_similarity is equal to the number of 2-item combinations in which the items' topics are the same\n",
    "    item_pair = items_shown[:, i]\n",
    "    topic_pair = bubble.item_topics[item_pair]\n",
    "    topic_similarity += (topic_pair[:,0] == topic_pair[:,1])\n",
    "\n",
    "slate_diversity = 1 - ((1 / (bubble.num_items_per_iter * (bubble.num_items_per_iter-1))) * topic_similarity)\n",
    "diversity = np.mean(slate_diversity)\n",
    "print(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Is diversity equal? \", measurements['diversity_metric'][-1] == diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bubble.get_measurements().keys():\n",
    "    print(key, bubble.get_measurements()[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing **all** evalmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeates **not** allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import NoveltyMetric, SerendipityMetric, DiversityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), NoveltyMetric(), SerendipityMetric(), DiversityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=1)\n",
    "measurements = bubble.get_measurements()\n",
    "\n",
    "for key in measurements:\n",
    "    print(key, \"-\", measurements[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=100)\n",
    "measurements = bubble.get_measurements()\n",
    "\n",
    "# print(\"Measurements for iters: \", str(np.arange(11) * 10))\n",
    "for key in measurements:\n",
    "    print(key, \"\\n\\t\", measurements[key][1::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeates **allowed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble = BubbleBurster(\n",
    "    # num_users=number_of_users,\n",
    "    # num_items=num_items,\n",
    "    # num_attributes=number_of_attributes,\n",
    "    item_topics=item_topics,\n",
    "    user_representation=user_representation,\n",
    "    item_representation=item_representation,\n",
    "    # actual_user_representation=users,\n",
    "    record_base_state=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wrapper.metrics import NoveltyMetric, SerendipityMetric, DiversityMetric\n",
    "\n",
    "bubble.add_metrics(MSEMeasurement(), NoveltyMetric(), SerendipityMetric(), DiversityMetric())\n",
    "print(\"These are the current metrics:\")\n",
    "print(bubble.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bubble.run(timesteps=100)\n",
    "measurements = bubble.get_measurements()\n",
    "\n",
    "# print(\"Measurements for iters: \", str(np.arange(11) * 10))\n",
    "for key in measurements:\n",
    "    print(key, \"\\n\\t\", measurements[key][1::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('rsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ff424ff1c077e7075cac16d9e16e601aa03a34fd87e9d6557d5c3f29437c7df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
